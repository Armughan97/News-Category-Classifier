{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a1699bb-b265-420d-8f2d-2dbce709be77",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f9045ae-144b-4d0a-a75a-76cab9af6d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.1\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "906eea48-01b2-4969-8b4a-44310d04788b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\CTN new\n",
      "[nltk_data]     Installation\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\CTN new\n",
      "[nltk_data]     Installation\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\CTN new\n",
      "[nltk_data]     Installation\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8000ab9d-4ced-40c3-8de5-23827762ea5c",
   "metadata": {},
   "source": [
    "# Part 1: Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ef89a1f-0563-4c68-aec6-ea71603fb589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b23de5c2-2569-4884-83f3-4288ea8836f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_sources = [\n",
    "    {\n",
    "        \"publisher\": \"BBC\",\n",
    "        \"base_url\": \"https://www.bbc.com\",\n",
    "        \"sections\": {\n",
    "            \"World\": \"/news/world\",\n",
    "            \"Arts\": \"/arts\",\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"publisher\": \"CNN\",\n",
    "        \"base_url\": \"https://edition.cnn.com\",\n",
    "        \"sections\": {\n",
    "            \"Politics\": \"/politics\",\n",
    "            \"Sports\": \"/sport\",\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"publisher\": \"CNBC\",\n",
    "        \"base_url\": \"https://www.cnbc.com\",\n",
    "        \"sections\": {\n",
    "            \"Health and Science\": \"/health-and-science\",\n",
    "            \"AI\": \"/ai-artificial-intelligence\",\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "news_articles = \"articles.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4249ab5f-8a05-4e50-a434-916a95322589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write news to csv\n",
    "def init_csv():\n",
    "    with open(OUTPUT_CSV, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\n",
    "            \"published_date\", \"headline\", \"publisher\", \"category\", \"article_content\", \"url\"\n",
    "        ])\n",
    "        writer.writeheader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21fa1a31-bcdc-4381-87c1-b114528f24db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch list of article URLs from a section page\n",
    "def get_article_links(source, category, page=1):\n",
    "    \"\"\"\n",
    "    Returns a list of absolute article URLs for a given section and page number.\n",
    "    \"\"\"\n",
    "    url = f\"{source['base_url']}{source['sections'][category]}\"\n",
    "    if page > 1:\n",
    "        url += f\"/{page}\"\n",
    "\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    links = []\n",
    "    for a in soup.select(\".gs-c-promo-heading a[href]\"):\n",
    "        href = a.get(\"href\")\n",
    "        if href.startswith(\"/news\"):\n",
    "            full = source['base_url'] + href\n",
    "            links.append(full)\n",
    "\n",
    "    return list(set(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f0ab241-14c3-428c-a291-0905c34985a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse individual article for metadata and content\n",
    "def parse_article(url, publisher, category):\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    # Published date (BBC example: time tag)\n",
    "    time_tag = soup.find(\"time\")\n",
    "    published_date = time_tag.get(\"datetime\") if time_tag else \"\"\n",
    "\n",
    "    # Headline\n",
    "    headline_tag = soup.find(\"h1\")\n",
    "    headline = headline_tag.get_text(strip=True) if headline_tag else \"\"\n",
    "\n",
    "    # Article content: collect all <p> text under article body\n",
    "    paragraphs = []\n",
    "    for p in soup.select(\"[property=articleBody] p, .ssrcss-uf6wea-RichTextComponentWrapper p\"):\n",
    "        paragraphs.append(p.get_text(strip=True))\n",
    "    article_content = \"\\n\".join(paragraphs)\n",
    "\n",
    "    return {\n",
    "        \"published_date\": published_date,\n",
    "        \"headline\": headline,\n",
    "        \"publisher\": publisher,\n",
    "        \"category\": category,\n",
    "        \"article_content\": article_content,\n",
    "        \"url\": url\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "622f7357-25b7-4284-a3da-39fbbc0d578b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop: iterate sources, categories, pages, and articles\n",
    "def scrape_all(max_pages=3, delay=1.0):\n",
    "    init_csv()\n",
    "    for source in SOURCES:\n",
    "        for category in source['sections']:\n",
    "            print(f\"Scraping {source['publisher']} - {category}\")\n",
    "            for page in range(1, max_pages + 1):\n",
    "                try:\n",
    "                    links = get_article_links(source, category, page)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to fetch page {page}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                for url in links:\n",
    "                    try:\n",
    "                        record = parse_article(url, source['publisher'], category)\n",
    "                        # Append to CSV\n",
    "                        with open(OUTPUT_CSV, mode=\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "                            writer = csv.DictWriter(f, fieldnames=record.keys())\n",
    "                            writer.writerow(record)\n",
    "                        print(f\"Saved: {record['headline']}\")\n",
    "                        time.sleep(delay)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing {url}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535ab156-f238-4d0c-b108-48f5a6b6a3ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (news-classifier)",
   "language": "python",
   "name": "news-classifier-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
