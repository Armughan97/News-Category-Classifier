{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a1699bb-b265-420d-8f2d-2dbce709be77",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9045ae-144b-4d0a-a75a-76cab9af6d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906eea48-01b2-4969-8b4a-44310d04788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8000ab9d-4ced-40c3-8de5-23827762ea5c",
   "metadata": {},
   "source": [
    "# Part 1: Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ef89a1f-0563-4c68-aec6-ea71603fb589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b23de5c2-2569-4884-83f3-4288ea8836f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_sources = [\n",
    "    {\n",
    "        \"publisher\": \"BBC\",\n",
    "        \"base_url\": \"https://www.bbc.com\",\n",
    "        \"sections\": {\n",
    "            \"World\": \"/news/world\",\n",
    "            \"Arts\": \"/arts\",\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"publisher\": \"CNN\",\n",
    "        \"base_url\": \"https://edition.cnn.com\",\n",
    "        \"sections\": {\n",
    "            \"Politics\": \"/politics\",\n",
    "            \"Sports\": \"/sport\",\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"publisher\": \"CNBC\",\n",
    "        \"base_url\": \"https://www.cnbc.com\",\n",
    "        \"sections\": {\n",
    "            \"Health and Science\": \"/health-and-science\",\n",
    "            \"AI\": \"/ai-artificial-intelligence\",\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "news_articles = \"articles.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4249ab5f-8a05-4e50-a434-916a95322589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write news to csv\n",
    "def init_csv():\n",
    "    with open(OUTPUT_CSV, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\n",
    "            \"published_date\", \"headline\", \"publisher\", \"category\", \"article_content\", \"url\"\n",
    "        ])\n",
    "        writer.writeheader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21fa1a31-bcdc-4381-87c1-b114528f24db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch list of article URLs from a section page\n",
    "def get_article_links(source, category, page=1):\n",
    "    \"\"\"\n",
    "    Returns a list of absolute article URLs for a given section and page number.\n",
    "    \"\"\"\n",
    "    url = f\"{source['base_url']}{source['sections'][category]}\"\n",
    "    if page > 1:\n",
    "        url += f\"/{page}\"\n",
    "\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    links = []\n",
    "    for a in soup.select(\".gs-c-promo-heading a[href]\"):\n",
    "        href = a.get(\"href\")\n",
    "        if href.startswith(\"/news\"):\n",
    "            full = source['base_url'] + href\n",
    "            links.append(full)\n",
    "\n",
    "    return list(set(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f0ab241-14c3-428c-a291-0905c34985a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse individual article for metadata and content\n",
    "def parse_article(url, publisher, category):\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    # Published date (BBC example: time tag)\n",
    "    time_tag = soup.find(\"time\")\n",
    "    published_date = time_tag.get(\"datetime\") if time_tag else \"\"\n",
    "\n",
    "    # Headline\n",
    "    headline_tag = soup.find(\"h1\")\n",
    "    headline = headline_tag.get_text(strip=True) if headline_tag else \"\"\n",
    "\n",
    "    # Article content: collect all <p> text under article body\n",
    "    paragraphs = []\n",
    "    for p in soup.select(\"[property=articleBody] p, .ssrcss-uf6wea-RichTextComponentWrapper p\"):\n",
    "        paragraphs.append(p.get_text(strip=True))\n",
    "    article_content = \"\\n\".join(paragraphs)\n",
    "\n",
    "    return {\n",
    "        \"published_date\": published_date,\n",
    "        \"headline\": headline,\n",
    "        \"publisher\": publisher,\n",
    "        \"category\": category,\n",
    "        \"article_content\": article_content,\n",
    "        \"url\": url\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "622f7357-25b7-4284-a3da-39fbbc0d578b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop: iterate sources, categories, pages, and articles\n",
    "def scrape_all(max_pages=3, delay=1.0):\n",
    "    init_csv()\n",
    "    for source in news_sources:\n",
    "        for category in source['sections']:\n",
    "            print(f\"Scraping {source['publisher']} - {category}\")\n",
    "            for page in range(1, max_pages + 1):\n",
    "                try:\n",
    "                    links = get_article_links(source, category, page)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to fetch page {page}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                for url in links:\n",
    "                    try:\n",
    "                        record = parse_article(url, source['publisher'], category)\n",
    "                        # Append to CSV\n",
    "                        with open(news_articles, mode=\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "                            writer = csv.DictWriter(f, fieldnames=record.keys())\n",
    "                            writer.writerow(record)\n",
    "                        print(f\"Saved: {record['headline']}\")\n",
    "                        time.sleep(delay)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing {url}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "535ab156-f238-4d0c-b108-48f5a6b6a3ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OUTPUT_CSV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Adjust max_pages to collect more articles per section\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mscrape_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_pages\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mscrape_all\u001b[39m\u001b[34m(max_pages, delay)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mscrape_all\u001b[39m(max_pages=\u001b[32m3\u001b[39m, delay=\u001b[32m1.0\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[43minit_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m source \u001b[38;5;129;01min\u001b[39;00m news_sources:\n\u001b[32m      5\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m category \u001b[38;5;129;01min\u001b[39;00m source[\u001b[33m'\u001b[39m\u001b[33msections\u001b[39m\u001b[33m'\u001b[39m]:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36minit_csv\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minit_csv\u001b[39m():\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[43mOUTPUT_CSV\u001b[49m, mode=\u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      4\u001b[39m         writer = csv.DictWriter(f, fieldnames=[\n\u001b[32m      5\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpublished_date\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mheadline\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpublisher\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcategory\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marticle_content\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33murl\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m         ])\n\u001b[32m      7\u001b[39m         writer.writeheader()\n",
      "\u001b[31mNameError\u001b[39m: name 'OUTPUT_CSV' is not defined"
     ]
    }
   ],
   "source": [
    "# Adjust max_pages to collect more articles per section\n",
    "scrape_all(max_pages=5, delay=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4459d03d-736d-48f4-854a-ac3d42455dbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (news-classifier)",
   "language": "python",
   "name": "news-classifier-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
