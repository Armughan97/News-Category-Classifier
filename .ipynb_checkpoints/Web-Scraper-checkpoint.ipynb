{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b99bd58b-dabe-4ec9-8914-fac013ec08a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e71219f-ca86-4ba6-911b-f4a5f4020520",
   "metadata": {},
   "source": [
    "## BBC Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1afdd237-2720-4aa2-a4c2-516341bf5416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# News Source\n",
    "news_sources = [\n",
    "    {\n",
    "        \"publisher\": \"BBC\",\n",
    "        \"base_url\": \"https://www.bbc.com\",\n",
    "        \"sections\": {\n",
    "            \"Business\": \"/business\",\n",
    "            \"Innovation\": \"/innovation\",\n",
    "            \"Culture\": \"/culture\",\n",
    "            \"Earth\": \"/future-planet\",\n",
    "            \"Entertainment\": \"/culture/entertainment-news\",\n",
    "            \"Arts\": \"/arts\",\n",
    "            \"Travel\": \"/travel\",\n",
    "        }\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b4feaa48-544b-4ba4-8c29-16c80596c915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize browser\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "wait = WebDriverWait(driver, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "326c86dd-96c1-4f25-8dd8-279b798e6efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException\n",
    "\n",
    "def close_popup(driver):\n",
    "    print(\"Attempting to close popup...\")\n",
    "    try:\n",
    "        # Wait for the iframe itself to be present\n",
    "        bbc_popup = WebDriverWait(driver, 5).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, 'iframe[id^=\"offer-0-\"]'))\n",
    "        )\n",
    "        \n",
    "        print(f\"Found BBC popup iframe. Switching to it (src: {bbc_popup.get_attribute('src')}).\")\n",
    "        driver.switch_to.frame(bbc_popup)\n",
    "        \n",
    "        # Now, within the iframe, find the close button.\n",
    "        close_button_in_iframe_selector = '.pn-article__close'\n",
    "        \n",
    "        close_btn_in_iframe = WebDriverWait(driver, 5).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, close_button_in_iframe_selector))\n",
    "        )\n",
    "        \n",
    "        if close_btn_in_iframe.is_displayed():\n",
    "            driver.execute_script(\"arguments[0].click();\", close_btn_in_iframe)\n",
    "            print(f\"Closed zephrIframeOutcome popup from within iframe using selector: {close_button_in_iframe_selector}\")\n",
    "            time.sleep(1)\n",
    "            driver.switch_to.default_content() # IMPORTANT: Switch back to main content\n",
    "            return True\n",
    "\n",
    "    except (TimeoutException, NoSuchElementException):\n",
    "        print(\"Popup or its close button not found within iframe.\")\n",
    "    except WebDriverException as e:\n",
    "        print(f\"WebDriver error interacting with Popup iframe: {e}\")\n",
    "    finally:\n",
    "        # Always attempt to switch back to default content, even if previous steps failed\n",
    "        # This prevents subsequent operations from failing if we are stuck in an iframe\n",
    "        try:\n",
    "            driver.switch_to.default_content()\n",
    "            print(\"Switched back to default content.\")\n",
    "        except Exception:\n",
    "            pass # Already on default content or other issue\n",
    "            \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c86b3b56-ecf7-42e6-8f5d-ca4c918e1bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_articles(url, category):\n",
    "    driver.get(url)\n",
    "    articles_data = []\n",
    "\n",
    "    # Let the page load\n",
    "    time.sleep(5)\n",
    "    # close popup if appears\n",
    "    close_popup(driver)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # 1. Find the parent container\n",
    "            container = wait.until(EC.presence_of_element_located(\n",
    "                (By.CSS_SELECTOR, 'div[class*=\"sc-c5803051-0\"]')  # Parent container\n",
    "            ))\n",
    "            \n",
    "            # 2. Find INDIVIDUAL ARTICLE CARDS within the container\n",
    "            cards = container.find_elements(By.CSS_SELECTOR, 'a[data-testid=\"internal-link\"]')  # Actual article elements\n",
    "            \n",
    "            # print(f\"Found {len(cards)} individual article cards\")\n",
    "            \n",
    "            # Extract data from each card\n",
    "            for card in cards:\n",
    "                try:\n",
    "                    # Extract url\n",
    "                    url = card.get_attribute('href')\n",
    "                    \n",
    "                    # Extract headline\n",
    "                    headline = card.find_element(By.CSS_SELECTOR, 'h2[data-testid=\"card-headline\"]').text\n",
    "                    \n",
    "                    articles_data.append({\n",
    "                        'headline': headline,\n",
    "                        'category': category,\n",
    "                        'url': url\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Skipping card due to error: {str(e)}\")\n",
    "                    continue\n",
    "                    \n",
    "            # print(f\"Collected {len(articles_data)} articles so far\")\n",
    "        \n",
    "            # Scroll to bottom to expose pagination controls\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(1)\n",
    "            # scroll up a bit\n",
    "            driver.execute_script(\"window.scrollBy(0, -300)\")\n",
    "            time.sleep(1)\n",
    "            \n",
    "            # Find and click next button\n",
    "            next_button = wait.until(EC.element_to_be_clickable(\n",
    "                (By.CSS_SELECTOR, 'button[data-testid=\"pagination-next-button\"]:not([disabled])')))\n",
    "            next_button.click()\n",
    "            # print(\"Clicked Next Page button\")\n",
    "            \n",
    "            # Reset scroll position for new page\n",
    "            driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "            time.sleep(1.5)\n",
    "            # gradual_scroll()  # Find \"More\" section again\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Pagination ended: {str(e)}\")\n",
    "            break\n",
    "\n",
    "    print(f\"Collected {len(articles_data)} articles\")\n",
    "    \n",
    "    return articles_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b2acbc67-f40a-4e72-bde6-a0409c013d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping BBC - Business From URL: https://www.bbc.com/business\n",
      "Attempting to close popup...\n",
      "Popup or its close button not found within iframe.\n",
      "Switched back to default content.\n",
      "Pagination ended: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x004FFC03+61635]\n",
      "\tGetHandleVerifier [0x004FFC44+61700]\n",
      "\t(No symbol) [0x003205D3]\n",
      "\t(No symbol) [0x0036899E]\n",
      "\t(No symbol) [0x00368D3B]\n",
      "\t(No symbol) [0x003B0E12]\n",
      "\t(No symbol) [0x0038D2E4]\n",
      "\t(No symbol) [0x003AE61B]\n",
      "\t(No symbol) [0x0038D096]\n",
      "\t(No symbol) [0x0035C840]\n",
      "\t(No symbol) [0x0035D6A4]\n",
      "\tGetHandleVerifier [0x00784523+2701795]\n",
      "\tGetHandleVerifier [0x0077FCA6+2683238]\n",
      "\tGetHandleVerifier [0x0079A9EE+2793134]\n",
      "\tGetHandleVerifier [0x005168C5+155013]\n",
      "\tGetHandleVerifier [0x0051CFAD+181357]\n",
      "\tGetHandleVerifier [0x00507458+92440]\n",
      "\tGetHandleVerifier [0x00507600+92864]\n",
      "\tGetHandleVerifier [0x004F1FF0+5296]\n",
      "\tBaseThreadInitThunk [0x76747BA9+25]\n",
      "\tRtlInitializeExceptionChain [0x77B1C0CB+107]\n",
      "\tRtlClearBits [0x77B1C04F+191]\n",
      "\n",
      "Collected 45 articles\n",
      "Scraping BBC - Innovation From URL: https://www.bbc.com/innovation\n",
      "Attempting to close popup...\n",
      "Found BBC popup iframe. Switching to it (src: https://buy-eu.piano.io/checkout/offer/show?displayMode=modal&templateId=OTVNGUND324P&offerId=OFCPZFU4G6UN&formNameByTermId=%7B%7D&hideCompletedFields=true&showCloseButton=false&experienceActionId=showOfferRZ7CIPNF3PUS72&offerType=purchase&experienceId=EXKR83WQK0Y0&activeMeters=%5B%7B%22meterName%22%3A%22RegMeter%22%2C%22views%22%3A2%2C%22viewsLeft%22%3A0%2C%22maxViews%22%3A2%2C%22totalViews%22%3A2%7D%5D&directCheckout=true&sessions=&widget=offer&iframeId=offer-0-ztk9F&url=https%3A%2F%2Fwww.bbc.com%2Finnovation&parentDualScreenLeft=9&parentDualScreenTop=9&parentWidth=1036&parentHeight=651&parentOuterHeight=798&aid=7I7hmRshpe&customVariables=%7B%22isMobileWebview%22%3Afalse%7D&browserId=mb39im5m5xcchvom&userState=anon&pianoIdUrl=https%3A%2F%2Fid.tinypass.com%2Fid%2F&userProvider=piano_id_lite&userToken=&customCookies=%7B%7D&hasLoginRequiredCallback=false&initMode=context&requestUserAuthForLinkedTerm=true&initTime=1928.4000000059605&logType=offerShow&width=1036&_qh=8ac44a41d0).\n",
      "Closed zephrIframeOutcome popup from within iframe using selector: .pn-article__close\n",
      "Switched back to default content.\n",
      "Pagination ended: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x004FFC03+61635]\n",
      "\tGetHandleVerifier [0x004FFC44+61700]\n",
      "\t(No symbol) [0x003205D3]\n",
      "\t(No symbol) [0x0036899E]\n",
      "\t(No symbol) [0x00368D3B]\n",
      "\t(No symbol) [0x003B0E12]\n",
      "\t(No symbol) [0x0038D2E4]\n",
      "\t(No symbol) [0x003AE61B]\n",
      "\t(No symbol) [0x0038D096]\n",
      "\t(No symbol) [0x0035C840]\n",
      "\t(No symbol) [0x0035D6A4]\n",
      "\tGetHandleVerifier [0x00784523+2701795]\n",
      "\tGetHandleVerifier [0x0077FCA6+2683238]\n",
      "\tGetHandleVerifier [0x0079A9EE+2793134]\n",
      "\tGetHandleVerifier [0x005168C5+155013]\n",
      "\tGetHandleVerifier [0x0051CFAD+181357]\n",
      "\tGetHandleVerifier [0x00507458+92440]\n",
      "\tGetHandleVerifier [0x00507600+92864]\n",
      "\tGetHandleVerifier [0x004F1FF0+5296]\n",
      "\tBaseThreadInitThunk [0x76747BA9+25]\n",
      "\tRtlInitializeExceptionChain [0x77B1C0CB+107]\n",
      "\tRtlClearBits [0x77B1C04F+191]\n",
      "\n",
      "Collected 99 articles\n",
      "Scraping BBC - Culture From URL: https://www.bbc.com/culture\n",
      "Attempting to close popup...\n",
      "Popup or its close button not found within iframe.\n",
      "Switched back to default content.\n",
      "Pagination ended: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x004FFC03+61635]\n",
      "\tGetHandleVerifier [0x004FFC44+61700]\n",
      "\t(No symbol) [0x003205D3]\n",
      "\t(No symbol) [0x0036899E]\n",
      "\t(No symbol) [0x00368D3B]\n",
      "\t(No symbol) [0x003B0E12]\n",
      "\t(No symbol) [0x0038D2E4]\n",
      "\t(No symbol) [0x003AE61B]\n",
      "\t(No symbol) [0x0038D096]\n",
      "\t(No symbol) [0x0035C840]\n",
      "\t(No symbol) [0x0035D6A4]\n",
      "\tGetHandleVerifier [0x00784523+2701795]\n",
      "\tGetHandleVerifier [0x0077FCA6+2683238]\n",
      "\tGetHandleVerifier [0x0079A9EE+2793134]\n",
      "\tGetHandleVerifier [0x005168C5+155013]\n",
      "\tGetHandleVerifier [0x0051CFAD+181357]\n",
      "\tGetHandleVerifier [0x00507458+92440]\n",
      "\tGetHandleVerifier [0x00507600+92864]\n",
      "\tGetHandleVerifier [0x004F1FF0+5296]\n",
      "\tBaseThreadInitThunk [0x76747BA9+25]\n",
      "\tRtlInitializeExceptionChain [0x77B1C0CB+107]\n",
      "\tRtlClearBits [0x77B1C04F+191]\n",
      "\n",
      "Collected 90 articles\n",
      "Scraping BBC - Earth From URL: https://www.bbc.com/future-planet\n",
      "Attempting to close popup...\n",
      "Popup or its close button not found within iframe.\n",
      "Switched back to default content.\n",
      "Pagination ended: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x004FFC03+61635]\n",
      "\tGetHandleVerifier [0x004FFC44+61700]\n",
      "\t(No symbol) [0x003205D3]\n",
      "\t(No symbol) [0x0036899E]\n",
      "\t(No symbol) [0x00368D3B]\n",
      "\t(No symbol) [0x003B0E12]\n",
      "\t(No symbol) [0x0038D2E4]\n",
      "\t(No symbol) [0x003AE61B]\n",
      "\t(No symbol) [0x0038D096]\n",
      "\t(No symbol) [0x0035C840]\n",
      "\t(No symbol) [0x0035D6A4]\n",
      "\tGetHandleVerifier [0x00784523+2701795]\n",
      "\tGetHandleVerifier [0x0077FCA6+2683238]\n",
      "\tGetHandleVerifier [0x0079A9EE+2793134]\n",
      "\tGetHandleVerifier [0x005168C5+155013]\n",
      "\tGetHandleVerifier [0x0051CFAD+181357]\n",
      "\tGetHandleVerifier [0x00507458+92440]\n",
      "\tGetHandleVerifier [0x00507600+92864]\n",
      "\tGetHandleVerifier [0x004F1FF0+5296]\n",
      "\tBaseThreadInitThunk [0x76747BA9+25]\n",
      "\tRtlInitializeExceptionChain [0x77B1C0CB+107]\n",
      "\tRtlClearBits [0x77B1C04F+191]\n",
      "\n",
      "Collected 99 articles\n",
      "Scraping BBC - Entertainment From URL: https://www.bbc.com/culture/entertainment-news\n",
      "Attempting to close popup...\n",
      "Popup or its close button not found within iframe.\n",
      "Switched back to default content.\n",
      "Pagination ended: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x004FFC03+61635]\n",
      "\tGetHandleVerifier [0x004FFC44+61700]\n",
      "\t(No symbol) [0x003205D3]\n",
      "\t(No symbol) [0x0036899E]\n",
      "\t(No symbol) [0x00368D3B]\n",
      "\t(No symbol) [0x003B0E12]\n",
      "\t(No symbol) [0x0038D2E4]\n",
      "\t(No symbol) [0x003AE61B]\n",
      "\t(No symbol) [0x0038D096]\n",
      "\t(No symbol) [0x0035C840]\n",
      "\t(No symbol) [0x0035D6A4]\n",
      "\tGetHandleVerifier [0x00784523+2701795]\n",
      "\tGetHandleVerifier [0x0077FCA6+2683238]\n",
      "\tGetHandleVerifier [0x0079A9EE+2793134]\n",
      "\tGetHandleVerifier [0x005168C5+155013]\n",
      "\tGetHandleVerifier [0x0051CFAD+181357]\n",
      "\tGetHandleVerifier [0x00507458+92440]\n",
      "\tGetHandleVerifier [0x00507600+92864]\n",
      "\tGetHandleVerifier [0x004F1FF0+5296]\n",
      "\tBaseThreadInitThunk [0x76747BA9+25]\n",
      "\tRtlInitializeExceptionChain [0x77B1C0CB+107]\n",
      "\tRtlClearBits [0x77B1C04F+191]\n",
      "\n",
      "Collected 100 articles\n",
      "Scraping BBC - Arts From URL: https://www.bbc.com/arts\n",
      "Attempting to close popup...\n",
      "Popup or its close button not found within iframe.\n",
      "Switched back to default content.\n",
      "Pagination ended: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x004FFC03+61635]\n",
      "\tGetHandleVerifier [0x004FFC44+61700]\n",
      "\t(No symbol) [0x003205D3]\n",
      "\t(No symbol) [0x0036899E]\n",
      "\t(No symbol) [0x00368D3B]\n",
      "\t(No symbol) [0x003B0E12]\n",
      "\t(No symbol) [0x0038D2E4]\n",
      "\t(No symbol) [0x003AE61B]\n",
      "\t(No symbol) [0x0038D096]\n",
      "\t(No symbol) [0x0035C840]\n",
      "\t(No symbol) [0x0035D6A4]\n",
      "\tGetHandleVerifier [0x00784523+2701795]\n",
      "\tGetHandleVerifier [0x0077FCA6+2683238]\n",
      "\tGetHandleVerifier [0x0079A9EE+2793134]\n",
      "\tGetHandleVerifier [0x005168C5+155013]\n",
      "\tGetHandleVerifier [0x0051CFAD+181357]\n",
      "\tGetHandleVerifier [0x00507458+92440]\n",
      "\tGetHandleVerifier [0x00507600+92864]\n",
      "\tGetHandleVerifier [0x004F1FF0+5296]\n",
      "\tBaseThreadInitThunk [0x76747BA9+25]\n",
      "\tRtlInitializeExceptionChain [0x77B1C0CB+107]\n",
      "\tRtlClearBits [0x77B1C04F+191]\n",
      "\n",
      "Collected 99 articles\n",
      "Scraping BBC - Travel From URL: https://www.bbc.com/travel\n",
      "Attempting to close popup...\n",
      "Popup or its close button not found within iframe.\n",
      "Switched back to default content.\n",
      "Pagination ended: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x004FFC03+61635]\n",
      "\tGetHandleVerifier [0x004FFC44+61700]\n",
      "\t(No symbol) [0x003205D3]\n",
      "\t(No symbol) [0x0036899E]\n",
      "\t(No symbol) [0x00368D3B]\n",
      "\t(No symbol) [0x003B0E12]\n",
      "\t(No symbol) [0x0038D2E4]\n",
      "\t(No symbol) [0x003AE61B]\n",
      "\t(No symbol) [0x0038D096]\n",
      "\t(No symbol) [0x0035C840]\n",
      "\t(No symbol) [0x0035D6A4]\n",
      "\tGetHandleVerifier [0x00784523+2701795]\n",
      "\tGetHandleVerifier [0x0077FCA6+2683238]\n",
      "\tGetHandleVerifier [0x0079A9EE+2793134]\n",
      "\tGetHandleVerifier [0x005168C5+155013]\n",
      "\tGetHandleVerifier [0x0051CFAD+181357]\n",
      "\tGetHandleVerifier [0x00507458+92440]\n",
      "\tGetHandleVerifier [0x00507600+92864]\n",
      "\tGetHandleVerifier [0x004F1FF0+5296]\n",
      "\tBaseThreadInitThunk [0x76747BA9+25]\n",
      "\tRtlInitializeExceptionChain [0x77B1C0CB+107]\n",
      "\tRtlClearBits [0x77B1C04F+191]\n",
      "\n",
      "Collected 81 articles\n",
      "Total articles:  613\n"
     ]
    }
   ],
   "source": [
    "# Scrape from all webpages\n",
    "articles_data = []\n",
    "for source in news_sources:\n",
    "    for category in source['sections']:\n",
    "        url = f\"{source['base_url']}{source['sections'][category]}\"\n",
    "        print(f\"Scraping {source['publisher']} - {category} From URL: {url}\")\n",
    "        articles_data.extend(scrape_articles(url, category))\n",
    "print(\"Total articles: \",len(articles_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e11caf88-d7e8-4812-a42f-68a673b2fc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "613\n"
     ]
    }
   ],
   "source": [
    "print(len(articles_data))\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9554e046-c6f9-486d-ade5-d9b8126ef5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 613 articles to CSV\n"
     ]
    }
   ],
   "source": [
    "# Save to CSV\n",
    "df = pd.DataFrame(articles_data)\n",
    "# df = df.drop_duplicates(subset=['url'])  # Remove duplicates\n",
    "df.to_csv('bbc_scraped_news_articles.csv', index=False)\n",
    "print(f\"Saved {len(df)} articles to CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b88334-7090-4bc1-8a8a-a8b269bdbb4e",
   "metadata": {},
   "source": [
    "## CNBC Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "de4f2bf2-f193-44fe-8736-69e20fe8ac86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# News Source\n",
    "news_sources = [\n",
    "    {\n",
    "        \"publisher\": \"CNBC\",\n",
    "        \"base_url\": \"https://www.cnbc.com\",\n",
    "        \"sections\": {\n",
    "            \"Economy\": \"/economy\",\n",
    "            \"Technology\": \"/technology\",\n",
    "            \"Travel\": \"/travel\",\n",
    "            \"Climate\": \"/climate\",\n",
    "            \"Entertainment\": \"/entertainment\",\n",
    "            \"Media\": \"/media\",\n",
    "            \"Life\": \"/life\",\n",
    "        }\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8f5dd805-6c62-4203-ac1c-95a28ef1fe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize browser\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "wait = WebDriverWait(driver, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "de4e2e4c-2e2c-49ea-9575-f2d6a0bd89e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException\n",
    "\n",
    "def close_popup(driver):\n",
    "    print(\"Attempting to close popup...\")\n",
    "    try:\n",
    "        # Wait for the iframe itself to be present\n",
    "        zephr_iframe = WebDriverWait(driver, 5).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, 'iframe.zephrIframeOutcome.OverlayApplicationPro'))\n",
    "        )\n",
    "        \n",
    "        print(f\"Found zephrIframeOutcome iframe. Switching to it (src: {zephr_iframe.get_attribute('src')}).\")\n",
    "        driver.switch_to.frame(zephr_iframe)\n",
    "        \n",
    "        # Now, within the iframe, find the close button.\n",
    "        # Based on the previous HTML you shared (which might now be *inside* this iframe)\n",
    "        # the button has data-testid=\"analytics-click\" and a class starting with \"CloseButton_closeButton\"\n",
    "        close_button_in_iframe_selector = 'button[data-testid=\"analytics-click\"][class*=\"CloseButton_closeButton\"]'\n",
    "        \n",
    "        close_btn_in_iframe = WebDriverWait(driver, 5).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, close_button_in_iframe_selector))\n",
    "        )\n",
    "        \n",
    "        if close_btn_in_iframe.is_displayed():\n",
    "            driver.execute_script(\"arguments[0].click();\", close_btn_in_iframe)\n",
    "            print(f\"Closed zephrIframeOutcome popup from within iframe using selector: {close_button_in_iframe_selector}\")\n",
    "            time.sleep(1)\n",
    "            driver.switch_to.default_content() # IMPORTANT: Switch back to main content\n",
    "            return True\n",
    "\n",
    "    except (TimeoutException, NoSuchElementException):\n",
    "        print(\"zephrIframeOutcome popup or its close button not found within iframe.\")\n",
    "    except WebDriverException as e:\n",
    "        print(f\"WebDriver error interacting with zephrIframeOutcome iframe: {e}\")\n",
    "    finally:\n",
    "        # Always attempt to switch back to default content, even if previous steps failed\n",
    "        # This prevents subsequent operations from failing if we are stuck in an iframe\n",
    "        try:\n",
    "            driver.switch_to.default_content()\n",
    "            print(\"Switched back to default content.\")\n",
    "        except Exception:\n",
    "            pass # Already on default content or other issue\n",
    "            \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "76c38371-f41a-40b9-a64f-9eebfaeb31e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_articles(url, category):\n",
    "    driver.get(url)\n",
    "    articles = []\n",
    "    num_articles = 100\n",
    "\n",
    "    while len(articles) < num_articles:\n",
    "        # Let the page load\n",
    "        time.sleep(10)\n",
    "        # close popup if appears\n",
    "        close_popup(driver)\n",
    "        \n",
    "        # Click to load more content (if needed)\n",
    "        load_more_button = driver.find_element(By.CLASS_NAME, \"LoadMoreButton-loadMore\")\n",
    "\n",
    "        # click load more a few times\n",
    "        for i in range(5):\n",
    "            if load_more_button.is_displayed():\n",
    "                load_more_button.click()\n",
    "                time.sleep(1)\n",
    "        \n",
    "        # Parse the page with BeautifulSoup\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        # Find all article links - adjust selector as needed\n",
    "        article_links = soup.select('a[href*=\"/20\"]')  # Links containing year-based URLs\n",
    "        \n",
    "        for link in article_links:\n",
    "            if len(articles) >= num_articles:\n",
    "                break\n",
    "                \n",
    "            headline = link.text.strip()\n",
    "            url = link['href']\n",
    "            \n",
    "            # Validate URL\n",
    "            if url.startswith('https://www.cnbc.com/') and headline:\n",
    "                articles.append({\n",
    "                    'headline': headline,\n",
    "                    'url': url,\n",
    "                    'category': category\n",
    "                })\n",
    "                # print(f\"Found article {len(articles)}: {headline}\")\n",
    "    \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b6f315ac-5106-463f-bfaa-c72f1afe177c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping CNBC - Economy From URL: https://www.cnbc.com/economy\n",
      "Attempting to close popup...\n",
      "zephrIframeOutcome popup or its close button not found within iframe.\n",
      "Switched back to default content.\n",
      "Scraping CNBC - Technology From URL: https://www.cnbc.com/technology\n",
      "Attempting to close popup...\n",
      "zephrIframeOutcome popup or its close button not found within iframe.\n",
      "Switched back to default content.\n",
      "Scraping CNBC - Travel From URL: https://www.cnbc.com/travel\n",
      "Attempting to close popup...\n",
      "Found zephrIframeOutcome iframe. Switching to it (src: https://zephr-templates.cnbc.com/components/OverlayApplicationPro?analyticsPageId=&analyticsTrackingId=&analyticsOutcomeId=zephr-overlay-ICMemorialDay2025INTL&analyticsOutcomeType=Overlay&analyticsOutcomeDetail=Zephr%7CDTCPromotion%7CInvestingClub%7CAcquisition%7COverlay%7CInternational%7CFreeArticles%7CMemorialDay2025&backgroundImageSrc=https%3A%2F%2Fimage.cnbcfm.com%2Fapi%2Fv1%2Fimage%2F108060557-1731095256460-IC_BFCM_2024_ZEPHR_OVERLAY_611x611_v01_1x.jpg%3Fv%3D1731095279&backgroundPosition=&backgroundColor=%23000000&articleTitle=SAVE+%24150+ON+CLUB+MEMBERSHIP&articleTitleFontColor=%23001e5a&articleTitlePrice=24999&articleTitlePriceFontColor=%23001e5a&articleTitleTextAlign=&articleSubtitle=FOR+1+YEAR&articleSubtitleFontColor=%23001e5a&articleSubtitleTextAlign=&articleLink=https%3A%2F%2Fwww.cnbc.com%2Finvestingclub%2Fsubscribe%2F%3FSummersale&articleTitleFontWeight=600&__source=Zephr%7CDTCPromotion%7CInvestingClub%7CAcquisition%7COverlay%7CInternational%7CFreeArticles%7CMemorialDay2025&tpcc=Zephr%7CDTCPromotion%7CInvestingClub%7CAcquisition%7COverlay%7CInternational%7CFreeArticles%7CMemorialDay2025&callToActionButtonText=CLAIM+OFFER&callToActionButtonTextColor=%23001e5a&callToActionButtonBackgroundColor=%23ffffff&callToActionButtonBorderColor=%23ffffff&callToActionButtonBorderRadius=&callToActionButtonBorderStyle=&callToActionButtonBorderWidth=&callToActionButtonFontFamily=proxima-nova&closeButtonColor=%23001e5a&showCloseButton=true&logoImageSrc=https%3A%2F%2Fimage.cnbcfm.com%2Fapi%2Fv1%2Fimage%2F107343979-1701878368029-club.svg%3Fv%3D1701878496%26h%3D500&logoImageAlt=&showLogoImage=true&url=&analyticsOutcomeSlug=transformation%2F145&windowTopLocationHref=https%3A%2F%2Fwww.cnbc.com%2Ftravel%2F).\n",
      "Closed zephrIframeOutcome popup from within iframe using selector: button[data-testid=\"analytics-click\"][class*=\"CloseButton_closeButton\"]\n",
      "Switched back to default content.\n",
      "Scraping CNBC - Climate From URL: https://www.cnbc.com/climate\n",
      "Attempting to close popup...\n",
      "Found zephrIframeOutcome iframe. Switching to it (src: https://zephr-templates.cnbc.com/components/OverlayApplicationPro?analyticsPageId=&analyticsTrackingId=&analyticsOutcomeId=zephr-overlay-ICMemorialDay2025INTL&analyticsOutcomeType=Overlay&analyticsOutcomeDetail=Zephr%7CDTCPromotion%7CInvestingClub%7CAcquisition%7COverlay%7CInternational%7CFreeArticles%7CMemorialDay2025&backgroundImageSrc=https%3A%2F%2Fimage.cnbcfm.com%2Fapi%2Fv1%2Fimage%2F108060557-1731095256460-IC_BFCM_2024_ZEPHR_OVERLAY_611x611_v01_1x.jpg%3Fv%3D1731095279&backgroundPosition=&backgroundColor=%23000000&articleTitle=SAVE+%24150+ON+CLUB+MEMBERSHIP&articleTitleFontColor=%23001e5a&articleTitlePrice=24999&articleTitlePriceFontColor=%23001e5a&articleTitleTextAlign=&articleSubtitle=FOR+1+YEAR&articleSubtitleFontColor=%23001e5a&articleSubtitleTextAlign=&articleLink=https%3A%2F%2Fwww.cnbc.com%2Finvestingclub%2Fsubscribe%2F%3FSummersale&articleTitleFontWeight=600&__source=Zephr%7CDTCPromotion%7CInvestingClub%7CAcquisition%7COverlay%7CInternational%7CFreeArticles%7CMemorialDay2025&tpcc=Zephr%7CDTCPromotion%7CInvestingClub%7CAcquisition%7COverlay%7CInternational%7CFreeArticles%7CMemorialDay2025&callToActionButtonText=CLAIM+OFFER&callToActionButtonTextColor=%23001e5a&callToActionButtonBackgroundColor=%23ffffff&callToActionButtonBorderColor=%23ffffff&callToActionButtonBorderRadius=&callToActionButtonBorderStyle=&callToActionButtonBorderWidth=&callToActionButtonFontFamily=proxima-nova&closeButtonColor=%23001e5a&showCloseButton=true&logoImageSrc=https%3A%2F%2Fimage.cnbcfm.com%2Fapi%2Fv1%2Fimage%2F107343979-1701878368029-club.svg%3Fv%3D1701878496%26h%3D500&logoImageAlt=&showLogoImage=true&url=&analyticsOutcomeSlug=transformation%2F145&windowTopLocationHref=https%3A%2F%2Fwww.cnbc.com%2Fclimate%2F).\n",
      "Closed zephrIframeOutcome popup from within iframe using selector: button[data-testid=\"analytics-click\"][class*=\"CloseButton_closeButton\"]\n",
      "Switched back to default content.\n",
      "Scraping CNBC - Entertainment From URL: https://www.cnbc.com/entertainment\n",
      "Attempting to close popup...\n",
      "zephrIframeOutcome popup or its close button not found within iframe.\n",
      "Switched back to default content.\n",
      "Scraping CNBC - Media From URL: https://www.cnbc.com/media\n",
      "Attempting to close popup...\n",
      "zephrIframeOutcome popup or its close button not found within iframe.\n",
      "Switched back to default content.\n",
      "Scraping CNBC - Life From URL: https://www.cnbc.com/life\n",
      "Attempting to close popup...\n",
      "zephrIframeOutcome popup or its close button not found within iframe.\n",
      "Switched back to default content.\n",
      "Total articles:  700\n"
     ]
    }
   ],
   "source": [
    "# Scrape from all webpages\n",
    "articles_data = []\n",
    "count = 0\n",
    "for source in news_sources:\n",
    "    for category in source['sections']:\n",
    "        count+= 1\n",
    "        if(count%100 == 0):\n",
    "            # exit and restart\n",
    "            driver.quit()\n",
    "            time.sleep(10)\n",
    "            # Initialize browser\n",
    "            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "            wait = WebDriverWait(driver, 15)\n",
    "        url = f\"{source['base_url']}{source['sections'][category]}\"\n",
    "        print(f\"Scraping {source['publisher']} - {category} From URL: {url}\")\n",
    "        articles_data.extend(scrape_articles(url, category))\n",
    "print(\"Total articles: \",len(articles_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "077e90fa-8eae-4ada-b4a0-51a1b3f160dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700\n"
     ]
    }
   ],
   "source": [
    "print(len(articles_data))\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7cf6320e-8c95-4339-bd4a-9bf07386c82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 700 articles to CSV\n"
     ]
    }
   ],
   "source": [
    "# Save to CSV\n",
    "df = pd.DataFrame(articles_data)\n",
    "# df = df.drop_duplicates(subset=['url'])  # Remove duplicates\n",
    "df.to_csv('cnbc_scraped_news_articles.csv', index=False)\n",
    "print(f\"Saved {len(df)} articles to CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ba1c4c-c568-4485-875c-e328ff23a317",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Reuters Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "6225dca4-513f-444b-8ff7-e404ca2fcbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# News Source\n",
    "news_sources = [\n",
    "    {\n",
    "        \"publisher\": \"Reuters\",\n",
    "        \"base_url\": \"https://www.reuters.com\",\n",
    "        \"sections\": {\n",
    "            \"Artificial Intelligence\": \"/technology/artificial-intelligence\",\n",
    "            \"Business\": \"/business\",\n",
    "            \"Weather\": \"/world/weather\",\n",
    "            \"Sports\": \"/sports\",\n",
    "            \"Lifestyle\": \"/lifestyle\",\n",
    "            \"Climate and Energy\": \"/sustainability/climate-energy\",\n",
    "            \"Government\": \"/legal/government\",\n",
    "        }\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "62c7d92f-5118-44fe-b9db-476d9420045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize browser\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "wait = WebDriverWait(driver, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "67b3558c-7637-438a-bd9e-98194e8dad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "def scrape_articles(url, category):\n",
    "    driver.get(url)\n",
    "    articles = []\n",
    "    num_articles = 100\n",
    "\n",
    "    while len(articles) < num_articles:\n",
    "        # Let the page load\n",
    "        time.sleep(10)\n",
    "\n",
    "        try:\n",
    "            # Click to load more articles (if needed)\n",
    "            load_more_button = driver.find_element(By.XPATH, \"//button//span[text()='Load more articles']\")\n",
    "\n",
    "            # click load more a few times\n",
    "            for i in range(5):\n",
    "                if load_more_button.is_displayed():\n",
    "                    load_more_button.click()\n",
    "                    time.sleep(1)\n",
    "                \n",
    "        except (TimeoutException):\n",
    "            pass\n",
    "        \n",
    "        # Parse the page with BeautifulSoup\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        # Find all article links - adjust selector as needed\n",
    "        article_links = soup.select('a[data-testid=\"TitleLink\"]') \n",
    "        \n",
    "        for link in article_links:\n",
    "            if len(articles) >= num_articles:\n",
    "                break\n",
    "                \n",
    "            # Extract headline from the nested span with data-testid=\"TitleHeading\"\n",
    "            headline_span = link.find('span', {'data-testid': 'TitleHeading'})\n",
    "            if headline_span:\n",
    "                headline = headline_span.text.strip()\n",
    "            else:\n",
    "                headline = link.text.strip()  # Fallback to full link text\n",
    "            \n",
    "            # Extract URL from href attribute\n",
    "            url = link.get('href', '')\n",
    "            \n",
    "            # Validate URL\n",
    "            if url.startswith('https://www.reuter.com/') and headline:\n",
    "                articles.append({\n",
    "                    'headline': headline,\n",
    "                    'url': url,\n",
    "                    'category': category\n",
    "                })\n",
    "                # print(f\"Found article {len(articles)}: {headline}\")\n",
    "    \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "87a85477-cbd9-44a6-8bf9-c6c5255df18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Reuters - Artificial Intelligence From URL: https://www.reuters.com/technology/artificial-intelligence\n"
     ]
    },
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//button//span[text()='Load more articles']\"}\n  (Session info: chrome=136.0.7103.114); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n\tGetHandleVerifier [0x0129FC03+61635]\n\tGetHandleVerifier [0x0129FC44+61700]\n\t(No symbol) [0x010C05D3]\n\t(No symbol) [0x0110899E]\n\t(No symbol) [0x01108D3B]\n\t(No symbol) [0x01150E12]\n\t(No symbol) [0x0112D2E4]\n\t(No symbol) [0x0114E61B]\n\t(No symbol) [0x0112D096]\n\t(No symbol) [0x010FC840]\n\t(No symbol) [0x010FD6A4]\n\tGetHandleVerifier [0x01524523+2701795]\n\tGetHandleVerifier [0x0151FCA6+2683238]\n\tGetHandleVerifier [0x0153A9EE+2793134]\n\tGetHandleVerifier [0x012B68C5+155013]\n\tGetHandleVerifier [0x012BCFAD+181357]\n\tGetHandleVerifier [0x012A7458+92440]\n\tGetHandleVerifier [0x012A7600+92864]\n\tGetHandleVerifier [0x01291FF0+5296]\n\tBaseThreadInitThunk [0x76747BA9+25]\n\tRtlInitializeExceptionChain [0x77B1C0CB+107]\n\tRtlClearBits [0x77B1C04F+191]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNoSuchElementException\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[555]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m         url = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource[\u001b[33m'\u001b[39m\u001b[33mbase_url\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msource[\u001b[33m'\u001b[39m\u001b[33msections\u001b[39m\u001b[33m'\u001b[39m][category]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mScraping \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource[\u001b[33m'\u001b[39m\u001b[33mpublisher\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m From URL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m         articles_data.extend(\u001b[43mscrape_articles\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTotal articles: \u001b[39m\u001b[33m\"\u001b[39m,\u001b[38;5;28mlen\u001b[39m(articles_data))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[554]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mscrape_articles\u001b[39m\u001b[34m(url, category)\u001b[39m\n\u001b[32m     10\u001b[39m time.sleep(\u001b[32m10\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# Click to load more articles (if needed)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     load_more_button = \u001b[43mdriver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m//button//span[text()=\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mLoad more articles\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# click load more a few times\u001b[39;00m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m5\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\News-Category-Classifier\\.venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:917\u001b[39m, in \u001b[36mWebDriver.find_element\u001b[39m\u001b[34m(self, by, value)\u001b[39m\n\u001b[32m    914\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m NoSuchElementException(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot locate relative element with: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mby.root\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    915\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m elements[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m917\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFIND_ELEMENT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43musing\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\News-Category-Classifier\\.venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:448\u001b[39m, in \u001b[36mWebDriver.execute\u001b[39m\u001b[34m(self, driver_command, params)\u001b[39m\n\u001b[32m    446\u001b[39m response = \u001b[38;5;28mself\u001b[39m.command_executor.execute(driver_command, params)\n\u001b[32m    447\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merror_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    449\u001b[39m     response[\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m._unwrap_value(response.get(\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    450\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\News-Category-Classifier\\.venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:232\u001b[39m, in \u001b[36mErrorHandler.check_response\u001b[39m\u001b[34m(self, response)\u001b[39m\n\u001b[32m    230\u001b[39m         alert_text = value[\u001b[33m\"\u001b[39m\u001b[33malert\u001b[39m\u001b[33m\"\u001b[39m].get(\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    231\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[31mNoSuchElementException\u001b[39m: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//button//span[text()='Load more articles']\"}\n  (Session info: chrome=136.0.7103.114); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n\tGetHandleVerifier [0x0129FC03+61635]\n\tGetHandleVerifier [0x0129FC44+61700]\n\t(No symbol) [0x010C05D3]\n\t(No symbol) [0x0110899E]\n\t(No symbol) [0x01108D3B]\n\t(No symbol) [0x01150E12]\n\t(No symbol) [0x0112D2E4]\n\t(No symbol) [0x0114E61B]\n\t(No symbol) [0x0112D096]\n\t(No symbol) [0x010FC840]\n\t(No symbol) [0x010FD6A4]\n\tGetHandleVerifier [0x01524523+2701795]\n\tGetHandleVerifier [0x0151FCA6+2683238]\n\tGetHandleVerifier [0x0153A9EE+2793134]\n\tGetHandleVerifier [0x012B68C5+155013]\n\tGetHandleVerifier [0x012BCFAD+181357]\n\tGetHandleVerifier [0x012A7458+92440]\n\tGetHandleVerifier [0x012A7600+92864]\n\tGetHandleVerifier [0x01291FF0+5296]\n\tBaseThreadInitThunk [0x76747BA9+25]\n\tRtlInitializeExceptionChain [0x77B1C0CB+107]\n\tRtlClearBits [0x77B1C04F+191]\n"
     ]
    }
   ],
   "source": [
    "# Scrape from all webpages\n",
    "articles_data = []\n",
    "count = 0\n",
    "for source in news_sources:\n",
    "    for category in source['sections']:\n",
    "        count+= 1\n",
    "        if(count%100 == 0):\n",
    "            # exit and restart\n",
    "            driver.quit()\n",
    "            time.sleep(10)\n",
    "            # Initialize browser\n",
    "            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "            wait = WebDriverWait(driver, 15)\n",
    "        url = f\"{source['base_url']}{source['sections'][category]}\"\n",
    "        print(f\"Scraping {source['publisher']} - {category} From URL: {url}\")\n",
    "        articles_data.extend(scrape_articles(url, category))\n",
    "print(\"Total articles: \",len(articles_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3faea0-6006-4686-9688-4380b2ea7aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(articles_data))\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737f9c9e-bf5f-459b-bb5a-9d1152599235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "df = pd.DataFrame(articles_data)\n",
    "df = df.drop_duplicates(subset=['url'])  # Remove duplicates\n",
    "df.to_csv('reuters_scraped_news_articles.csv', index=False)\n",
    "print(f\"Saved {len(df)} articles to CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9f4483-35a9-4c1c-9985-18b64df16b7e",
   "metadata": {},
   "source": [
    "# Combined Articles Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0fb9ff52-6c21-4b88-8153-ac66ff2e9931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_details(url, publisher):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract publication date\n",
    "        published_date = None\n",
    "        \n",
    "        # Common meta tag approach\n",
    "        meta_date = soup.find('meta', property='article:published_time')\n",
    "        if meta_date:\n",
    "            published_date = meta_date['content']\n",
    "        else:\n",
    "            # Fallback for different date formats\n",
    "            time_tag = soup.find('time')\n",
    "            if time_tag and 'datetime' in time_tag.attrs:\n",
    "                published_date = time_tag['datetime']\n",
    "            elif time_tag:\n",
    "                published_date = time_tag.text.strip()\n",
    "\n",
    "        # Convert to standard format\n",
    "        if published_date:\n",
    "            try:\n",
    "                published_date = datetime.fromisoformat(published_date.replace('Z', '+00:00')).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Date format conversion failed: {str(e)}\")\n",
    "                print(f\"⏳ Keeping original date format: {published_date}\")\n",
    "\n",
    "        # Extract article content based on publisher\n",
    "        article_content = []\n",
    "        \n",
    "        if publisher.lower() == 'bbc':\n",
    "            content_blocks = soup.find_all('div', {'data-component': 'text-block'})\n",
    "            article_content = [block.text.strip() for block in content_blocks]\n",
    "        elif publisher.lower() == 'cnbc':\n",
    "            content_blocks = soup.select('div.group p')\n",
    "            article_content = [block.text.strip() for block in content_blocks]\n",
    "        \n",
    "        # Join paragraphs with newlines\n",
    "        full_content = '\\n\\n'.join(article_content)\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        return {\n",
    "            'published_date': published_date,\n",
    "            'article_content': full_content\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error scraping {url}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4b54891b-08dc-4657-9b18-8a51894a2f5d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Articles: 100%|██████████████████████████████████████████████████████| 1313/1313 [26:59<00:00,  1.23s/article]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 Saving progress...\n",
      "\n",
      "✅ Completed: 1313/1313 articles successfully scraped\n",
      "💾 Saving final file...\n",
      "⏱️ Total processing time: 1619.58 seconds\n"
     ]
    }
   ],
   "source": [
    "# list of all publishers and their data file\n",
    "publishers = [\n",
    "    {\n",
    "    'name':'bbc',\n",
    "    'file':'bbc_scraped_news_articles.csv',\n",
    "    },\n",
    "    {\n",
    "    'name':'cnbc',\n",
    "    'file':'cnbc_scraped_news_articles.csv',\n",
    "    },\n",
    "]\n",
    "\n",
    "# Load existing data\n",
    "df = pd.DataFrame()\n",
    "for publisher in publishers:\n",
    "    # n = 5\n",
    "    # temp = pd.read_csv(publisher['file'], nrows = n)\n",
    "    temp = pd.read_csv(publisher['file'])\n",
    "    temp['publisher'] = publisher['name']\n",
    "    df = pd.concat([df, temp],ignore_index=True)\n",
    "\n",
    "# Add new columns\n",
    "df['published_date'] = None\n",
    "df['article_content'] = None\n",
    "# df['publisher'] = None\n",
    "\n",
    "success = 0\n",
    "total = len(df)\n",
    "start_time = time.time()\n",
    "\n",
    "# Scrape additional data with rate limiting\n",
    "try:\n",
    "    # Configure tqdm progress bar\n",
    "    with tqdm(total=total, desc=\"Scraping Articles\", unit=\"article\") as pbar:\n",
    "        for index, row in df.iterrows():\n",
    "            result = extract_article_details(row['url'], row['publisher'])\n",
    "            \n",
    "            if result:\n",
    "                df.at[index, 'published_date'] = result['published_date']\n",
    "                df.at[index, 'article_content'] = result['article_content']\n",
    "                df.at[index, 'publisher'] = row['publisher']\n",
    "                success += 1\n",
    "                \n",
    "            pbar.update(1)\n",
    "            time.sleep(0.25)  # Maintain polite delay\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n⚠️ User interrupted the process! Saving current progress...\")\n",
    "\n",
    "finally:\n",
    "    # Reorder columns and exclude URL\n",
    "    new_column_order = [\n",
    "        'published_date',\n",
    "        'headline', \n",
    "        'publisher',\n",
    "        'article_content',\n",
    "        'category'\n",
    "    ]\n",
    "    \n",
    "    # Create new DataFrame with desired columns\n",
    "    final_df = df[new_column_order]\n",
    "    \n",
    "    # Save updated CSV\n",
    "    print(\"\\n💾 Saving progress...\")\n",
    "    final_df.to_csv('combined_news_articles_data.csv', index=False)\n",
    "    # print(f\"💽 Saved {success_count} articles to combined_news_articles_data.csv\")\n",
    "    # print(f\"📈 Success rate: {success_count/processed_count:.2%}\") if processed_count > 0 else \"\"\n",
    "    # Final output\n",
    "    print(f\"\\n✅ Completed: {success}/{total} articles successfully scraped\")\n",
    "    print(f\"💾 Saving final file...\")\n",
    "    print(f\"⏱️ Total processing time: {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aea086-c72a-4d0c-901c-ee576c824509",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (news-classifier)",
   "language": "python",
   "name": "news-classifier-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
